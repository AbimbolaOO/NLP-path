{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineTranslationModel.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyBawkbSn3lC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYVFCx2BgjlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q3tHFUG18GJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchtext\n",
        "from torchtext.datasets import TranslationDataset, Multi30k\n",
        "from torchtext.data import Field, BucketIterator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRhotxR62GXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 25\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True # To make our results reproduciable elsewhere"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WGonyMd7CPv",
        "colab_type": "code",
        "outputId": "21a9c705-0dd7-4359-ad6d-2944f8155a79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz#egg=de_core_news_sm==2.1.0\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz (11.1MB)\n",
            "\u001b[K     |████████████████████████████████| 11.1MB 624kB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: de-core-news-sm\n",
            "  Building wheel for de-core-news-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for de-core-news-sm: filename=de_core_news_sm-2.1.0-cp36-none-any.whl size=11073065 sha256=12093b89a1ae8387bf9bfc88f4676ca5e0744439931c82556783bca789448175\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-lkwes1jn/wheels/b4/8b/5e/d2ce5d2756ca95de22f50f68299708009a4aafda2aea79c4e4\n",
            "Successfully built de-core-news-sm\n",
            "Installing collected packages: de-core-news-sm\n",
            "Successfully installed de-core-news-sm-2.1.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OA9nn-nXDEFQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "spacy_german = spacy.load('de')\n",
        "spacy_english = spacy.load('en')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VW6UcSFFnOE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def german_tokenizer(text):\n",
        "    r\"\"\"\n",
        "    This create a sequence of german tokens and reverses it\n",
        "    Note :: Reversing the strings helps our encoder to perform better \n",
        "            in Machine Translation\n",
        "    \"\"\"\n",
        "    \n",
        "    return [token.text for token in spacy_german.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCe2N63WIe_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def english_tokenizer(text):\n",
        "    r\"\"\"\n",
        "    This create a sequence of english tokens\n",
        "    Note :: We don't the everse the text here because we are giving this \n",
        "            to our decoder \n",
        "    \"\"\"\n",
        "    \n",
        "    return [token.text for token in spacy_english.tokenizer(text)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zgs8LWXBHepI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DE defines the field used for german\n",
        "DE = torchtext.data.Field(tokenize=german_tokenizer,\n",
        "                             init_token='<sos>',\n",
        "                             eos_token = '<eos>',\n",
        "                             lower=True)\n",
        "\n",
        "# EN defined the firld used for english\n",
        "EN = torchtext.data.Field(tokenize=english_tokenizer,\n",
        "                             init_token='<sos>',\n",
        "                             eos_token = '<eos>',\n",
        "                             lower=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS873dtmHeZB",
        "colab_type": "code",
        "outputId": "22a5878c-dcf6-4e27-9e19-57fd6458e1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# The data is been divided into train, validation and test data\n",
        "# exts (extensions) for the languages are represented with the turple below\n",
        "# The fields are represented in the vector below\n",
        "train_data, validation_data, test_data = torchtext.datasets.Multi30k.splits(\n",
        "                                                            exts = ('.de', '.en'),\n",
        "                                                            fields = (DE, EN))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\rtraining.tar.gz:   0%|          | 0.00/1.21M [00:00<?, ?B/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading training.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "training.tar.gz: 100%|██████████| 1.21M/1.21M [00:00<00:00, 7.01MB/s]\n",
            "validation.tar.gz: 100%|██████████| 46.3k/46.3k [00:00<00:00, 1.80MB/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading validation.tar.gz\n",
            "downloading mmt_task1_test2016.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "mmt_task1_test2016.tar.gz: 100%|██████████| 66.2k/66.2k [00:00<00:00, 1.69MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIvR5HeFYgWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This builds_vocabs method helps to build word vocabulary and remove \n",
        "# the less frequent words \n",
        "DE.build_vocab(train_data, min_freq = 2)\n",
        "EN.build_vocab(train_data, min_freq = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZDc0VX8Ygjv",
        "colab_type": "code",
        "outputId": "ea80d505-78df-475f-ca8e-b39952cb57a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# num_german_word_types is the number of unique words in our corpus or vocabulary\n",
        "num_german_word_types = len(DE.vocab)\n",
        "\n",
        "# num_english_word_types is the number of unique words in our corpus or vocabulary\n",
        "num_english_word_types = len(EN.vocab)\n",
        "\n",
        "print(f'The number of german wordtypes {num_german_word_types}')\n",
        "print(f'The number of english wordtypes {num_english_word_types}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of german wordtypes 7855\n",
            "The number of english wordtypes 5893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mi-b2L_b6Fo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJ-SHvbzfFPy",
        "colab_type": "code",
        "outputId": "9ca97085-1371-431d-fb5d-402a88b021e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gvp2MO6TazXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size_ = 128\n",
        "train_iterator, validation_iterator, test_iterator = torchtext.data.BucketIterator.splits(\n",
        "                                                                    datasets=(train_data, validation_data, test_data),\n",
        "                                                                    batch_size = batch_size_,\n",
        "                                                                    # device uses the gpu if available\n",
        "                                                                    device=device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T4nIHA2iHgWw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 word_types, # Number of unique words in the vocabulary \n",
        "                 embed_dim, #This is the embedding dimesion and it's usually 300\n",
        "                 hidden_size, # This is the number of hidden unit in our elman network\n",
        "                 number_layers, # This the number of LSTM layers stacked on eachother\n",
        "                 dropout_ # This is a regularizer\n",
        "                 ):\n",
        "        \n",
        "        super(Encoder, self).__init__()\n",
        "        # Instancialising the embedding layers\n",
        "        self.embed = nn.Embedding(word_types, embed_dim)\n",
        "        # Instancialising the long short term memory\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, number_layers, dropout=dropout_)\n",
        "        # Instancialising the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_)\n",
        "        \n",
        "    def forward(self, source_language):\n",
        "        \n",
        "        # The langauge is embedded and to reduce dimensionality and regularizer was aplied\n",
        "        embeddings = self.dropout(self.embed(source_language))\n",
        "        ##################################################################\n",
        "        #                                                                #\n",
        "        #  From the pytorch documentations we are given this values as:  #\n",
        "        #  input = (seq_len, batch, input_size)                          #\n",
        "        #  ::Note:: `num_directions` is 1 here because it's not a BiRNN  #\n",
        "        #  h_0 = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #  c_0 = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #  output = (seq_len, batch, num_directions * hidden_size)       #\n",
        "        #  h_n = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #  c_n = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #                                                                #\n",
        "        ##################################################################\n",
        "        # Note only the input(embeddings) to the LSTM are given hence\n",
        "        # (h_0, c_0) will default to zeros at the initial time state\n",
        "        # (h_n, c_n) these are the  hidden and cell state at the next time step\n",
        "        output, (h_n, c_n) = self.lstm(embeddings)\n",
        "        \n",
        "        return h_n, c_n\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMWEINMgvPdK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, \n",
        "                 word_types, # Number of unique words in the vocabulary \n",
        "                 embed_dim, #This is the embedding dimesion and it's usually 300\n",
        "                 hidden_size, # This is the number of hidden unit in our elman network\n",
        "                 number_layers, # This the number of LSTM layers stacked on eachother\n",
        "                 dropout_, # This is a regularizer\n",
        "                 output_dim # The out dimension of the classifier\n",
        "                 ):\n",
        "        \n",
        "        super(Decoder, self).__init__()\n",
        "        # Output dimension\n",
        "        self.output_dim = output_dim\n",
        "        # Instancialising the embedding layers\n",
        "        self.embed = nn.Embedding(word_types, embed_dim)\n",
        "        # Instancialising the long short term memory\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_size, number_layers, dropout=dropout_)\n",
        "        # Instancialising the dropout layer\n",
        "        self.dropout = nn.Dropout(dropout_)\n",
        "        # The classification layer\n",
        "        self.out = nn.Linear(hidden_size, output_dim)\n",
        "        \n",
        "        \n",
        "    def forward(self, target_language, hidden, cell):\n",
        "        \"\"\"\n",
        "        Arguments:\n",
        "            target_language: The language we want to translate to and it also the decoder input\n",
        "            hidden: The source hidden state\n",
        "            cell: The source cell state\n",
        "        \"\"\"\n",
        "        # target_language [batch_size]\n",
        "        target_language = target_language.unsqueeze(0) # this gives our target_language a sequence lenght of 1 i.e [1, batch_size]\n",
        "        \n",
        "        # The langauge is embedded to reduce dimensionality and regularizer was aplied\n",
        "        embeddings = self.dropout(self.embed(target_language))\n",
        "        ##################################################################\n",
        "        #                                                                #\n",
        "        #  From the pytorch documentations we are given this values as:  #\n",
        "        #  input = (seq_len, batch, input_size)                          #\n",
        "        #  ::Note:: `num_directions` is 1 here because it's not a BiRNN  #\n",
        "        #  h_0 = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #  c_0 = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #  output = (seq_len, batch, num_directions * hidden_size)       #\n",
        "        #  h_n = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #  c_n = (num_layers * num_directions, batch, hidden_size)       #\n",
        "        #                                                                #\n",
        "        ##################################################################\n",
        "        # output has a dim => [seq_len=1, batch_size, hidden_size]\n",
        "        output, (hidden, cell) = self.lstm(embeddings, (hidden, cell))\n",
        "        # output has a dim => [batch_size, hidden_size]\n",
        "        output = output.squeeze(0)\n",
        "        # prediction has a dim => [batch_size, output_dim]\n",
        "        prediction = self.out(output)\n",
        "        return prediction, hidden, cell\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPH_kcEU1FVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sequence to Sequence\n",
        "class Seq2seq(nn.Module):\n",
        "    \n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src_lang, trg_lang, teacher_forcing_ratio=0.5):\n",
        "        # src_lang as a dim => [seq_len, batch_size]\n",
        "        # trg_lang as a dim => [seq_len, batch_size]\n",
        "        # teacher_forcing_ratio is the probability for using the ground truth\n",
        "        # here we use a value of 0.5 i.e 50% of the time we will be pass in the the ground truth at the next time step\n",
        "        \n",
        "        batch_size = trg_lang.shape[1]\n",
        "        seq_len = trg_lang.shape[0]\n",
        "        trg_vocab_size = num_english_word_types\n",
        "        \n",
        "        #zeros tesor with dim=> [seq_len, batch_size, trg_vocab_size] used for storing the output of the decode \n",
        "        decoder_outputs_tensor = torch.zeros(seq_len, batch_size, trg_vocab_size).to(device)\n",
        "        \n",
        "        # The context vector is equivalent to the last hidden and cell state of the encoder in the case of LSTM\n",
        "        hidden, cell = self.encoder(src_lang)\n",
        "        \n",
        "        #first input to the decoder is the <sos> token i.e state of sentence token\n",
        "        decoder_input = trg_lang[0,:]\n",
        "        \n",
        "        # We pass each batch of tokens in the sequence of token one after the other till we get to <eos>\n",
        "        for t in range(1, seq_len):\n",
        "            decoder_output, decoder_hidden, decoder_cell = self.decoder(decoder_input, hidden, cell)\n",
        "            decoder_outputs_tensor[t] = decoder_output\n",
        "            teacher_forcing = np.random.random() < teacher_forcing_ratio\n",
        "            # torch.max() return a named_tuple of containing(values, indices) of the max value\n",
        "            top1 = decoder_output.max(1).indices\n",
        "            # top1 has a dim => [128]\n",
        "            decoder_input = (trg_lang[t] if teacher_forcing else top1)\n",
        "            \n",
        "        return decoder_outputs_tensor\n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_a_VdemHgvB",
        "colab_type": "code",
        "outputId": "b929d946-3038-4c3f-e5d8-46772b3ed51f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Initializing the encoder\n",
        "word_types = num_german_word_types\n",
        "embed_dim = 300\n",
        "hidden_size = int(1.5*embed_dim)\n",
        "number_layers = 2\n",
        "dropout_ = 0.2\n",
        "encoder = Encoder(word_types, embed_dim, hidden_size, number_layers, dropout_).to(device)\n",
        "\n",
        "# Initializing the decoder\n",
        "word_types = num_english_word_types\n",
        "embed_dim = 300\n",
        "hidden_size = int(1.5*embed_dim)\n",
        "number_layers = 2\n",
        "dropout_ = 0.2\n",
        "output_dim = num_english_word_types\n",
        "decoder = Decoder(word_types, embed_dim, hidden_size, number_layers, dropout_, output_dim).to(device)\n",
        "\n",
        "# Intializing the Seq2seq model\n",
        "s2s_model = Seq2seq(encoder, decoder).to(device)\n",
        "\n",
        "# Weights Initailization\n",
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "s2s_model.apply(init_weights)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2seq(\n",
              "  (encoder): Encoder(\n",
              "    (embed): Embedding(7855, 300)\n",
              "    (lstm): LSTM(300, 450, num_layers=2, dropout=0.2)\n",
              "    (dropout): Dropout(p=0.2)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embed): Embedding(5893, 300)\n",
              "    (lstm): LSTM(300, 450, num_layers=2, dropout=0.2)\n",
              "    (dropout): Dropout(p=0.2)\n",
              "    (out): Linear(in_features=450, out_features=5893, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C4nL_7v3yl-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Optimizer \n",
        "# Here we use the default learing rate lr=1e-3\n",
        "optimizer = torch.optim.Adam(s2s_model.parameters())\n",
        "\n",
        "# Loss Function\n",
        "# DE.vocab.stoi retun an orderdict with key value pairs of word and numerical representation\n",
        "# PAD_IDX takes the numerical representation for <pad> \n",
        "PAD_IDX  = DE.vocab.stoi['<pad>']\n",
        "\n",
        "# ignore_index (int, optional): Specifies a target value that is ignored\n",
        "# and does not contribute to the input gradient. When :attr:`size_average` is\n",
        "#  ``True``, the loss is averaged over non-ignored targets.\n",
        "\n",
        "criteria = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjjwZhjjGXqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainer(model, n_epoch, tran_iter, valid_iter, optim, criterion, clip, model_path):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    batch_train_loss = 0\n",
        "    batch_valid_loss = 0\n",
        "    save_checker = np.inf\n",
        "    for i in range(1, n_epoch+1):\n",
        "        \n",
        "        for batch in tran_iter:\n",
        "\n",
        "            # src dim => [seq_len, batch_size]\n",
        "            # trg dim => [seq_len, batch_size]\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # output = [seq_len, batch_size, len(num_german_word_types)]\n",
        "            output = model(src, trg)\n",
        "\n",
        "            # trg dim => [(seq_len - 1)*batch_size]\n",
        "            trg = trg[1:].view(-1)\n",
        "            # output dim => [(seq_len - 1)*batch_size, output_dim]\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "\n",
        "            train_loss = criterion(output, trg)\n",
        "            train_loss.backward()\n",
        "            # We use gradient clip to prevent exploding gradients\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            batch_train_loss = train_loss.item()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            \n",
        "        with torch.no_grad():\n",
        "            \n",
        "            for batch in valid_iter:\n",
        "\n",
        "                # src dim => [seq_len, batch_size]\n",
        "                # trg dim => [seq_len, batch_size]\n",
        "                src = batch.src\n",
        "                trg = batch.trg\n",
        "\n",
        "                # output = [seq_len, batch_size, len(num_german_word_types)]\n",
        "                output = model(src, trg)\n",
        "\n",
        "                # trg dim => [(seq_len - 1)*batch_size]\n",
        "                trg = trg[1:].view(-1)\n",
        "                # output dim => [(seq_len - 1)*batch_size, output_dim]\n",
        "                output = output[1:].view(-1, output_dim)\n",
        "\n",
        "                valid_loss = criterion(output, trg)\n",
        "                batch_valid_loss = valid_loss.item()\n",
        "                \n",
        "        print('[{0}/{1}] \\t Training Loss: {2} \\t Validation Loss {3}'\n",
        "              .format(i, n_epoch, round(batch_train_loss, 5), round(batch_valid_loss, 5)))\n",
        "        \n",
        "        if batch_valid_loss < save_checker:\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "            save_checker = batch_valid_loss\n",
        "            print(\".............................SAVING MODEL.............................\")\n",
        "         \n",
        "             "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rMMbTgcQGXnC",
        "colab_type": "code",
        "outputId": "1c44d144-8097-43a6-ec23-78d3e3d25dd1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "clip_ = 1\n",
        "num_epoch = 200\n",
        "model_path_ = \"tranduction_model.pt\"\n",
        "trainer(s2s_model, num_epoch, train_iterator, validation_iterator, optimizer, criteria, clip_, model_path_)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1/200] \t Training Loss: 2.56228 \t Validation Loss 4.14235\n",
            ".............................SAVING MODEL.............................\n",
            "[2/200] \t Training Loss: 2.75277 \t Validation Loss 3.95206\n",
            ".............................SAVING MODEL.............................\n",
            "[3/200] \t Training Loss: 2.09442 \t Validation Loss 4.14673\n",
            "[4/200] \t Training Loss: 2.3739 \t Validation Loss 3.96643\n",
            "[5/200] \t Training Loss: 2.03582 \t Validation Loss 4.03432\n",
            "[6/200] \t Training Loss: 2.09294 \t Validation Loss 4.24987\n",
            "[7/200] \t Training Loss: 1.93962 \t Validation Loss 4.25913\n",
            "[8/200] \t Training Loss: 1.79305 \t Validation Loss 4.2229\n",
            "[9/200] \t Training Loss: 1.83787 \t Validation Loss 4.30477\n",
            "[10/200] \t Training Loss: 1.82551 \t Validation Loss 4.28059\n",
            "[11/200] \t Training Loss: 1.55142 \t Validation Loss 4.31253\n",
            "[12/200] \t Training Loss: 1.70666 \t Validation Loss 4.36655\n",
            "[13/200] \t Training Loss: 2.05888 \t Validation Loss 4.15026\n",
            "[14/200] \t Training Loss: 1.90912 \t Validation Loss 4.35416\n",
            "[15/200] \t Training Loss: 1.74476 \t Validation Loss 4.41631\n",
            "[16/200] \t Training Loss: 1.44695 \t Validation Loss 4.67524\n",
            "[17/200] \t Training Loss: 1.67561 \t Validation Loss 4.63497\n",
            "[18/200] \t Training Loss: 2.18189 \t Validation Loss 4.36812\n",
            "[19/200] \t Training Loss: 1.37243 \t Validation Loss 4.57859\n",
            "[20/200] \t Training Loss: 1.31105 \t Validation Loss 4.52908\n",
            "[21/200] \t Training Loss: 1.61368 \t Validation Loss 4.70625\n",
            "[22/200] \t Training Loss: 1.45556 \t Validation Loss 4.66039\n",
            "[23/200] \t Training Loss: 1.27537 \t Validation Loss 4.74213\n",
            "[24/200] \t Training Loss: 1.33005 \t Validation Loss 4.33961\n",
            "[25/200] \t Training Loss: 1.54887 \t Validation Loss 4.63786\n",
            "[26/200] \t Training Loss: 1.35378 \t Validation Loss 4.63204\n",
            "[27/200] \t Training Loss: 1.15733 \t Validation Loss 4.38258\n",
            "[28/200] \t Training Loss: 1.18546 \t Validation Loss 4.45474\n",
            "[29/200] \t Training Loss: 1.24243 \t Validation Loss 4.75424\n",
            "[30/200] \t Training Loss: 1.20654 \t Validation Loss 4.71858\n",
            "[31/200] \t Training Loss: 1.74386 \t Validation Loss 4.44304\n",
            "[32/200] \t Training Loss: 1.67527 \t Validation Loss 5.40412\n",
            "[33/200] \t Training Loss: 1.19724 \t Validation Loss 4.52634\n",
            "[34/200] \t Training Loss: 1.71085 \t Validation Loss 5.03455\n",
            "[35/200] \t Training Loss: 1.32163 \t Validation Loss 4.52741\n",
            "[36/200] \t Training Loss: 1.5263 \t Validation Loss 4.81319\n",
            "[37/200] \t Training Loss: 0.86907 \t Validation Loss 4.74524\n",
            "[38/200] \t Training Loss: 1.27982 \t Validation Loss 4.7494\n",
            "[39/200] \t Training Loss: 0.99226 \t Validation Loss 5.12182\n",
            "[40/200] \t Training Loss: 1.54967 \t Validation Loss 5.02248\n",
            "[41/200] \t Training Loss: 1.42183 \t Validation Loss 5.07861\n",
            "[42/200] \t Training Loss: 0.99866 \t Validation Loss 4.87589\n",
            "[43/200] \t Training Loss: 1.48089 \t Validation Loss 4.6141\n",
            "[44/200] \t Training Loss: 1.34333 \t Validation Loss 5.14926\n",
            "[45/200] \t Training Loss: 0.86273 \t Validation Loss 5.12683\n",
            "[46/200] \t Training Loss: 1.03269 \t Validation Loss 4.84458\n",
            "[47/200] \t Training Loss: 1.00707 \t Validation Loss 4.9569\n",
            "[48/200] \t Training Loss: 0.98043 \t Validation Loss 4.87173\n",
            "[49/200] \t Training Loss: 0.67109 \t Validation Loss 5.58542\n",
            "[50/200] \t Training Loss: 1.11578 \t Validation Loss 5.14242\n",
            "[51/200] \t Training Loss: 1.27276 \t Validation Loss 4.9295\n",
            "[52/200] \t Training Loss: 0.78367 \t Validation Loss 5.10321\n",
            "[53/200] \t Training Loss: 0.86715 \t Validation Loss 5.05301\n",
            "[54/200] \t Training Loss: 1.17914 \t Validation Loss 5.16873\n",
            "[55/200] \t Training Loss: 1.11254 \t Validation Loss 4.75528\n",
            "[56/200] \t Training Loss: 0.94276 \t Validation Loss 5.24086\n",
            "[57/200] \t Training Loss: 1.40462 \t Validation Loss 4.61455\n",
            "[58/200] \t Training Loss: 0.9774 \t Validation Loss 5.15418\n",
            "[59/200] \t Training Loss: 0.83408 \t Validation Loss 5.10091\n",
            "[60/200] \t Training Loss: 1.17892 \t Validation Loss 5.32754\n",
            "[61/200] \t Training Loss: 0.88748 \t Validation Loss 4.78952\n",
            "[62/200] \t Training Loss: 0.87817 \t Validation Loss 5.59661\n",
            "[63/200] \t Training Loss: 1.16817 \t Validation Loss 4.95056\n",
            "[64/200] \t Training Loss: 0.93602 \t Validation Loss 4.97569\n",
            "[65/200] \t Training Loss: 0.98287 \t Validation Loss 5.21886\n",
            "[66/200] \t Training Loss: 0.88736 \t Validation Loss 5.85875\n",
            "[67/200] \t Training Loss: 1.02119 \t Validation Loss 5.45865\n",
            "[68/200] \t Training Loss: 0.95858 \t Validation Loss 5.58082\n",
            "[69/200] \t Training Loss: 0.72174 \t Validation Loss 5.11366\n",
            "[70/200] \t Training Loss: 1.22127 \t Validation Loss 5.58789\n",
            "[71/200] \t Training Loss: 0.83543 \t Validation Loss 5.57882\n",
            "[72/200] \t Training Loss: 0.95942 \t Validation Loss 5.3972\n",
            "[73/200] \t Training Loss: 0.71582 \t Validation Loss 5.92013\n",
            "[74/200] \t Training Loss: 1.12473 \t Validation Loss 5.47204\n",
            "[75/200] \t Training Loss: 1.45938 \t Validation Loss 4.94497\n",
            "[76/200] \t Training Loss: 0.90962 \t Validation Loss 5.25467\n",
            "[77/200] \t Training Loss: 0.6295 \t Validation Loss 5.42907\n",
            "[78/200] \t Training Loss: 0.76366 \t Validation Loss 5.54411\n",
            "[79/200] \t Training Loss: 0.98376 \t Validation Loss 5.06914\n",
            "[80/200] \t Training Loss: 1.58389 \t Validation Loss 5.09495\n",
            "[81/200] \t Training Loss: 0.93835 \t Validation Loss 5.08058\n",
            "[82/200] \t Training Loss: 0.80167 \t Validation Loss 5.18031\n",
            "[83/200] \t Training Loss: 1.32087 \t Validation Loss 5.26987\n",
            "[84/200] \t Training Loss: 1.06375 \t Validation Loss 5.31534\n",
            "[85/200] \t Training Loss: 0.84768 \t Validation Loss 5.44638\n",
            "[86/200] \t Training Loss: 0.94505 \t Validation Loss 5.09988\n",
            "[87/200] \t Training Loss: 0.66654 \t Validation Loss 4.94825\n",
            "[88/200] \t Training Loss: 0.76527 \t Validation Loss 5.42676\n",
            "[89/200] \t Training Loss: 0.84969 \t Validation Loss 5.41899\n",
            "[90/200] \t Training Loss: 0.65422 \t Validation Loss 5.36154\n",
            "[91/200] \t Training Loss: 0.84077 \t Validation Loss 5.49555\n",
            "[92/200] \t Training Loss: 0.83137 \t Validation Loss 5.68428\n",
            "[93/200] \t Training Loss: 1.06753 \t Validation Loss 5.60377\n",
            "[94/200] \t Training Loss: 0.72873 \t Validation Loss 5.17022\n",
            "[95/200] \t Training Loss: 0.72149 \t Validation Loss 5.35993\n",
            "[96/200] \t Training Loss: 0.96918 \t Validation Loss 5.38564\n",
            "[97/200] \t Training Loss: 1.49713 \t Validation Loss 5.30233\n",
            "[98/200] \t Training Loss: 0.79571 \t Validation Loss 5.584\n",
            "[99/200] \t Training Loss: 0.83139 \t Validation Loss 5.6587\n",
            "[100/200] \t Training Loss: 0.82437 \t Validation Loss 5.27734\n",
            "[101/200] \t Training Loss: 1.53069 \t Validation Loss 5.1506\n",
            "[102/200] \t Training Loss: 1.10739 \t Validation Loss 5.60791\n",
            "[103/200] \t Training Loss: 0.52634 \t Validation Loss 5.65191\n",
            "[104/200] \t Training Loss: 0.75583 \t Validation Loss 5.40612\n",
            "[105/200] \t Training Loss: 0.60391 \t Validation Loss 5.43308\n",
            "[106/200] \t Training Loss: 0.63055 \t Validation Loss 5.6525\n",
            "[107/200] \t Training Loss: 0.65178 \t Validation Loss 5.23924\n",
            "[108/200] \t Training Loss: 0.86984 \t Validation Loss 5.39275\n",
            "[109/200] \t Training Loss: 0.58182 \t Validation Loss 5.24066\n",
            "[110/200] \t Training Loss: 0.75709 \t Validation Loss 4.7575\n",
            "[111/200] \t Training Loss: 0.6576 \t Validation Loss 5.38542\n",
            "[112/200] \t Training Loss: 0.57872 \t Validation Loss 5.11167\n",
            "[113/200] \t Training Loss: 0.837 \t Validation Loss 5.37635\n",
            "[114/200] \t Training Loss: 0.57683 \t Validation Loss 5.63485\n",
            "[115/200] \t Training Loss: 0.7446 \t Validation Loss 5.22219\n",
            "[116/200] \t Training Loss: 0.95141 \t Validation Loss 5.45985\n",
            "[117/200] \t Training Loss: 0.84628 \t Validation Loss 5.56186\n",
            "[118/200] \t Training Loss: 0.66623 \t Validation Loss 6.04805\n",
            "[119/200] \t Training Loss: 0.68221 \t Validation Loss 4.98077\n",
            "[120/200] \t Training Loss: 0.85203 \t Validation Loss 5.42902\n",
            "[121/200] \t Training Loss: 0.55286 \t Validation Loss 5.40766\n",
            "[122/200] \t Training Loss: 0.8144 \t Validation Loss 5.52932\n",
            "[123/200] \t Training Loss: 0.88302 \t Validation Loss 5.29087\n",
            "[124/200] \t Training Loss: 0.48438 \t Validation Loss 5.47179\n",
            "[125/200] \t Training Loss: 0.69584 \t Validation Loss 5.76025\n",
            "[126/200] \t Training Loss: 0.66626 \t Validation Loss 5.22175\n",
            "[127/200] \t Training Loss: 0.70851 \t Validation Loss 5.08989\n",
            "[128/200] \t Training Loss: 0.67792 \t Validation Loss 5.22464\n",
            "[129/200] \t Training Loss: 0.75432 \t Validation Loss 5.63821\n",
            "[130/200] \t Training Loss: 0.54955 \t Validation Loss 5.68149\n",
            "[131/200] \t Training Loss: 0.76364 \t Validation Loss 5.30317\n",
            "[132/200] \t Training Loss: 0.569 \t Validation Loss 5.4697\n",
            "[133/200] \t Training Loss: 0.89988 \t Validation Loss 5.47561\n",
            "[134/200] \t Training Loss: 0.84906 \t Validation Loss 5.43588\n",
            "[135/200] \t Training Loss: 0.66753 \t Validation Loss 5.32136\n",
            "[136/200] \t Training Loss: 0.90403 \t Validation Loss 5.79655\n",
            "[137/200] \t Training Loss: 0.77242 \t Validation Loss 5.78369\n",
            "[138/200] \t Training Loss: 0.71325 \t Validation Loss 5.8067\n",
            "[139/200] \t Training Loss: 0.62905 \t Validation Loss 5.09136\n",
            "[140/200] \t Training Loss: 0.92422 \t Validation Loss 5.74985\n",
            "[141/200] \t Training Loss: 0.80483 \t Validation Loss 5.78365\n",
            "[142/200] \t Training Loss: 0.6284 \t Validation Loss 5.46033\n",
            "[143/200] \t Training Loss: 0.72802 \t Validation Loss 5.14515\n",
            "[144/200] \t Training Loss: 0.75457 \t Validation Loss 5.45416\n",
            "[145/200] \t Training Loss: 0.78626 \t Validation Loss 5.67984\n",
            "[146/200] \t Training Loss: 0.51517 \t Validation Loss 5.46745\n",
            "[147/200] \t Training Loss: 0.39849 \t Validation Loss 5.50414\n",
            "[148/200] \t Training Loss: 0.92888 \t Validation Loss 5.48596\n",
            "[149/200] \t Training Loss: 0.69568 \t Validation Loss 5.67375\n",
            "[150/200] \t Training Loss: 0.66579 \t Validation Loss 5.4351\n",
            "[151/200] \t Training Loss: 0.66546 \t Validation Loss 5.33324\n",
            "[152/200] \t Training Loss: 0.5655 \t Validation Loss 5.60096\n",
            "[153/200] \t Training Loss: 0.55725 \t Validation Loss 5.13592\n",
            "[154/200] \t Training Loss: 0.87784 \t Validation Loss 5.36396\n",
            "[155/200] \t Training Loss: 0.92935 \t Validation Loss 5.04522\n",
            "[156/200] \t Training Loss: 0.76227 \t Validation Loss 5.53721\n",
            "[157/200] \t Training Loss: 0.48815 \t Validation Loss 5.40852\n",
            "[158/200] \t Training Loss: 0.54117 \t Validation Loss 5.62007\n",
            "[159/200] \t Training Loss: 0.98899 \t Validation Loss 5.5215\n",
            "[160/200] \t Training Loss: 0.95828 \t Validation Loss 5.30796\n",
            "[161/200] \t Training Loss: 0.68768 \t Validation Loss 5.41482\n",
            "[162/200] \t Training Loss: 0.62993 \t Validation Loss 5.33672\n",
            "[163/200] \t Training Loss: 0.63207 \t Validation Loss 5.75732\n",
            "[164/200] \t Training Loss: 0.67748 \t Validation Loss 5.92092\n",
            "[165/200] \t Training Loss: 0.61381 \t Validation Loss 5.46284\n",
            "[166/200] \t Training Loss: 0.99577 \t Validation Loss 5.34809\n",
            "[167/200] \t Training Loss: 0.63834 \t Validation Loss 5.18224\n",
            "[168/200] \t Training Loss: 0.53272 \t Validation Loss 5.59174\n",
            "[169/200] \t Training Loss: 0.9257 \t Validation Loss 6.6656\n",
            "[170/200] \t Training Loss: 0.68367 \t Validation Loss 5.98626\n",
            "[171/200] \t Training Loss: 0.6339 \t Validation Loss 5.81307\n",
            "[172/200] \t Training Loss: 0.59615 \t Validation Loss 5.22656\n",
            "[173/200] \t Training Loss: 0.55122 \t Validation Loss 5.86736\n",
            "[174/200] \t Training Loss: 0.5575 \t Validation Loss 5.33742\n",
            "[175/200] \t Training Loss: 0.61798 \t Validation Loss 5.94548\n",
            "[176/200] \t Training Loss: 0.50183 \t Validation Loss 6.20941\n",
            "[177/200] \t Training Loss: 0.74564 \t Validation Loss 5.36054\n",
            "[178/200] \t Training Loss: 0.51095 \t Validation Loss 5.14723\n",
            "[179/200] \t Training Loss: 0.50926 \t Validation Loss 6.1679\n",
            "[180/200] \t Training Loss: 0.61316 \t Validation Loss 5.76837\n",
            "[181/200] \t Training Loss: 0.61053 \t Validation Loss 5.57237\n",
            "[182/200] \t Training Loss: 0.49866 \t Validation Loss 5.86884\n",
            "[183/200] \t Training Loss: 0.43923 \t Validation Loss 6.29714\n",
            "[184/200] \t Training Loss: 0.69532 \t Validation Loss 5.52439\n",
            "[185/200] \t Training Loss: 1.43905 \t Validation Loss 5.56443\n",
            "[186/200] \t Training Loss: 0.5624 \t Validation Loss 5.44243\n",
            "[187/200] \t Training Loss: 0.51124 \t Validation Loss 5.49839\n",
            "[188/200] \t Training Loss: 0.82878 \t Validation Loss 5.55878\n",
            "[189/200] \t Training Loss: 0.65414 \t Validation Loss 5.71497\n",
            "[190/200] \t Training Loss: 0.84881 \t Validation Loss 5.32724\n",
            "[191/200] \t Training Loss: 0.87932 \t Validation Loss 5.51229\n",
            "[192/200] \t Training Loss: 1.04302 \t Validation Loss 5.40448\n",
            "[193/200] \t Training Loss: 0.53917 \t Validation Loss 5.92349\n",
            "[194/200] \t Training Loss: 0.80116 \t Validation Loss 5.37986\n",
            "[195/200] \t Training Loss: 0.56364 \t Validation Loss 5.49221\n",
            "[196/200] \t Training Loss: 0.5896 \t Validation Loss 5.5575\n",
            "[197/200] \t Training Loss: 0.65361 \t Validation Loss 5.4621\n",
            "[198/200] \t Training Loss: 0.54069 \t Validation Loss 6.20907\n",
            "[199/200] \t Training Loss: 0.75707 \t Validation Loss 5.72107\n",
            "[200/200] \t Training Loss: 0.77021 \t Validation Loss 5.89346\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}